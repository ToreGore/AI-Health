{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, re, math\n",
    "import time\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Dense, MaxPooling2D\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import AvgPool2D, GlobalAveragePooling2D, MaxPool2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import ReLU, concatenate\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np                                    \n",
    "import pandas as pd \n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow.keras.backend as K\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default strategy for CPU and single GPU\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available:  1\n",
      "REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"GPU\"\n",
    "if DEVICE == \"TPU\":\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        print(\"Could not connect to TPU\")\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        try:\n",
    "            print(\"initializing  TPU ...\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "            print(\"TPU initialized\")\n",
    "        except _:\n",
    "            print(\"failed to initialize TPU\")\n",
    "    else:\n",
    "        DEVICE = \"GPU\"\n",
    "\n",
    "if DEVICE != \"TPU\":\n",
    "    print(\"Using default strategy for CPU and single GPU\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "if DEVICE == \"GPU\":\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    print(physical_devices)\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    \n",
    "\n",
    "AUTO     = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Network Architectures:\n",
    "### Generalized Sequental Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialNet:\n",
    "    def __init__(self, input_shape,n_classes, filters=32):\n",
    "        self.input_shape = input_shape\n",
    "        self.n_classes = n_classes\n",
    "        self.filters = filters\n",
    "        self.model = tf.keras.Sequential()\n",
    "        \n",
    "        # self.topology_dict = topology_dict # contains the layers = which function is called when.\n",
    "    def build_self(self, topology_list):\n",
    "        self.model.add(tf.keras.Input(shape=self.input_shape))\n",
    "        for layer in topology_list:\n",
    "            self.model.add(layer)\n",
    "        self.model.add(Dense(self.n_classes))\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy() \n",
    "        self.model.compile(optimizer=opt,loss=loss,metrics=['accuracy'])\n",
    "        return self.model\n",
    "        \n",
    "        # self.model.add()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet(input_shape, n_classes, activation = \"softmax\", filters = 32):\n",
    "  # Same sequence for each convolutional block after the input\n",
    "    def bn_rl_conv(x, filters, kernel=3, strides=2):\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        x = Conv2D(filters, kernel, strides=strides, padding = \"same\")(x)\n",
    "        return x\n",
    "  \n",
    "    def dense_block(x, repetition):\n",
    "    # Each dense block has 2 convolutions with 1x1 and 3x3 kernels\n",
    "    # Each block is run for the 6,12,24,16\n",
    "        for _ in range(repetition):\n",
    "            y = bn_rl_conv(x, 4*filters)    # Every 1x1 convolutions has 4-times the number of filters\n",
    "            y = bn_rl_conv(y, filters, 3)   # But 3x3 filters are oly present once\n",
    "        return x\n",
    "  \n",
    "    def transition_layer(x):\n",
    "        # Remove channels to half of the existing channels \n",
    "        x = bn_rl_conv(x, K.int_shape(x)[-1]//2)        # 1x1 convolution layer\n",
    "        x = AvgPool2D(2, strides=2, padding='same')(x)  # 2x2 average poolling layer with strid of 2\n",
    "        return x\n",
    "\n",
    "    input = Input(input_shape)\n",
    "    # 1st convolution block with 64 filters of size 7x7 & a stride of 2:\n",
    "    x = Conv2D(64, 7, strides = 2, padding = \"same\")(input)\n",
    "    # Max pooling laxer with 3x3 max pooling & stride of 2\n",
    "    x = MaxPool2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    # Run 4-times trough the 6, 12, 24, 16 repetitions\n",
    "    for repetition in [6,12,24,16]:\n",
    "        d = dense_block(x, repetition)\n",
    "        x = transition_layer(d)    \n",
    "\n",
    "    x = GlobalAveragePooling2D()(d)\n",
    "    # Final dense output layer\n",
    "    output = Dense(n_classes, activation=activation)(x)\n",
    "    model = Model(input, output)\n",
    "    return model\n",
    "\n",
    "## Compare to DenseNet121 from keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data\n",
    "### Functions to prepare/load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=15, linewidth=80)\n",
    "CLASSES = [0,1]\n",
    "IMAGE_SIZE= [512,512]\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    \"\"\" Count number of data items in TFRecord file\n",
    "    INPUT: filenames:   names of tfrecord files\n",
    "    OUTPUT:             how many files are stored in the tfrecord file \n",
    "    \"\"\"\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n",
    "         for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "def decode_image(image_data, augment=False):\n",
    "    \"\"\" Decode image, convert to float and resize\n",
    "    INPUT:  image_data: tensor of image stored in TFRecords\n",
    "            augment:    (boolean) for possible augmentation\n",
    "    OUTPUT:             decoded, resized image\n",
    "    \"\"\"\n",
    "    img = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    img = tf.cast(img, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
    "    if augment:\n",
    "        img = transform(img,DIM=dim)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        #img = tf.image.random_hue(img, 0.01)\n",
    "        img = tf.image.random_saturation(img, 0.7, 1.3)\n",
    "        img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "        img = tf.image.random_brightness(img, 0.1)\n",
    "    img = tf.reshape(img, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
    "    return img\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    \"\"\" Read labeled data from tfrecord and extract image and label\n",
    "    INPUT:  flexible message type read from TFREcords\n",
    "    OUTPUT: dataset of (image, label) pairs\n",
    "    \"\"\"\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        'image'                        : tf.io.FixedLenFeature([], tf.string),\n",
    "        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n",
    "        'patient_id'                   : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'anatom_site_general'          : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'diagnosis'                    : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'target'                       : tf.io.FixedLenFeature([], tf.int64)\n",
    "    } \n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    print(\"Image: \", image)\n",
    "    label = example['target']\n",
    "    return image, label\n",
    "\n",
    "def load_dataset(filenames, labeled=True, repeat=False, shuffle=False):\n",
    "    \"\"\" Read from TFRecords. For optimal performance, reading from multiple files at once and disregarding data order. \n",
    "    INPUT:  filenames:  (array<string>) paths to the tfrecord files\n",
    "            labeled:    (boolean) if the data is labeled or not (currently not implemented)\n",
    "            repeat      (boolean) if the data should be repeated\n",
    "            shuffle:    (boolean) if the data should be shuffled\n",
    "    OUTPUT: labelled dataset containing all features from TFRecord file\n",
    "    \"\"\"\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.cache()\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(2048)\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "        dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset\n",
    "\n",
    "def get_dataset(FILENAMES, repeat=False, shuffle=False):\n",
    "    dataset = load_dataset(FILENAMES, True, repeat, shuffle)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEARNING RATE\n",
    "def get_lr_callback(batch_size=32):\n",
    "    lr_start   = 0.000005\n",
    "    lr_max     = 0.00000125 * batch_size\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "            \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "            \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "            \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    return lr_callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Layer list of Sequential Networks\n",
    "### Generate a list of Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list = [\n",
    "    Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    Conv2D(16, (3, 3), activation=\"relu\"),\n",
    "    Conv2D(8, (3, 3), activation=\"relu\"),\n",
    "    Dense(32, activation=\"sigmoid\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate VGG list of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_list = [\n",
    "  Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "  Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "  MaxPooling2D((2,2), strides=(2,2)),\n",
    "  Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "  Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "  MaxPooling2D((2,2), strides=(2,2)),\n",
    "  Conv2D(256, (3, 3), activation=\"relu\"),\n",
    "  Conv2D(256, (3, 3), activation=\"relu\"),\n",
    "  Conv2D(256, (3, 3), activation=\"relu\"),\n",
    "  Conv2D(256, (3, 3), activation=\"relu\"),\n",
    "  MaxPooling2D((2,2), strides=(2,2)),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4142 train images\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "VERBOSE = 2\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "#TRAINING_FILENAMES = tf.io.gfile.glob(os.getcwd()+'/train*.tfrec')\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(\"./tfrecs/\" + \"train*.tfrec\")\n",
    "VALIDATION_FILENAMES = [TRAINING_FILENAMES.pop()]\n",
    "print('There are %i train images'%count_data_items(TRAINING_FILENAMES))\n",
    "#tqdm_callback = tf.callbacks.TQDMProgressBar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 512, 512, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 256, 256, 64)      9472      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "re_lu_12 (ReLU)              (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 64, 64, 32)        18464     \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_37 (ReLU)              (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 16, 16, 16)        4624      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 8, 8, 16)          64        \n",
      "_________________________________________________________________\n",
      "re_lu_86 (ReLU)              (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_87 (Conv2D)           (None, 4, 4, 8)           1160      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 2, 2, 8)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 34,186\n",
      "Trainable params: 33,962\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "# Save best model\n",
    "sv = tf.keras.callbacks.ModelCheckpoint('dense_model_cath.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n",
    "\n",
    "with strategy.scope():\n",
    "    dense_model = densenet((512,512,3), 2, \"softmax\")\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy() \n",
    "    dense_model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "    dense_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Image:  Tensor(\"Reshape:0\", shape=(512, 512, 3), dtype=float32)\n",
      "Image:  Tensor(\"Reshape:0\", shape=(512, 512, 3), dtype=float32)\n",
      "Epoch 1/10\n",
      "129/129 - 41s - loss: 0.2043 - accuracy: 0.9195 - val_loss: 0.2172 - val_accuracy: 0.9070\n",
      "Epoch 2/10\n",
      "129/129 - 40s - loss: 0.2018 - accuracy: 0.9221 - val_loss: 0.2168 - val_accuracy: 0.9081\n",
      "Epoch 3/10\n",
      "129/129 - 41s - loss: 0.2088 - accuracy: 0.9166 - val_loss: 0.2132 - val_accuracy: 0.9086\n",
      "Epoch 4/10\n",
      "129/129 - 41s - loss: 0.2004 - accuracy: 0.9243 - val_loss: 0.2616 - val_accuracy: 0.9065\n",
      "Epoch 5/10\n",
      "129/129 - 45s - loss: 0.2044 - accuracy: 0.9168 - val_loss: 0.2211 - val_accuracy: 0.9145\n",
      "Epoch 6/10\n",
      "129/129 - 46s - loss: 0.1890 - accuracy: 0.9291 - val_loss: 0.2697 - val_accuracy: 0.9065\n",
      "Epoch 7/10\n",
      "129/129 - 41s - loss: 0.1974 - accuracy: 0.9204 - val_loss: 0.2134 - val_accuracy: 0.9118\n",
      "Epoch 8/10\n",
      "129/129 - 41s - loss: 0.1969 - accuracy: 0.9202 - val_loss: 0.2283 - val_accuracy: 0.9140\n",
      "Epoch 9/10\n",
      "129/129 - 41s - loss: 0.1786 - accuracy: 0.9349 - val_loss: 0.2127 - val_accuracy: 0.9140\n",
      "Epoch 10/10\n",
      "129/129 - 41s - loss: 0.1875 - accuracy: 0.9240 - val_loss: 0.2283 - val_accuracy: 0.9140\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "history = dense_model.fit(\n",
    "    get_dataset(TRAINING_FILENAMES, True, True), \n",
    "    epochs=10, \n",
    "    callbacks = [sv, get_lr_callback(BATCH_SIZE)], \n",
    "    steps_per_epoch=count_data_items(TRAINING_FILENAMES)/BATCH_SIZE, \n",
    "    validation_data=get_dataset(VALIDATION_FILENAMES, False, False), \n",
    "    verbose=VERBOSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Loading best model...')\n",
    "dense_model.load_weights('dense_model_cath.h5')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training VGG Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_124 (Conv2D)          (None, 510, 510, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_125 (Conv2D)          (None, 508, 508, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 254, 254, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_126 (Conv2D)          (None, 252, 252, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_127 (Conv2D)          (None, 250, 250, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 125, 125, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_128 (Conv2D)          (None, 123, 123, 256)     295168    \n",
      "_________________________________________________________________\n",
      "conv2d_129 (Conv2D)          (None, 121, 121, 256)     590080    \n",
      "_________________________________________________________________\n",
      "conv2d_130 (Conv2D)          (None, 119, 119, 256)     590080    \n",
      "_________________________________________________________________\n",
      "conv2d_131 (Conv2D)          (None, 117, 117, 256)     590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 58, 58, 2)         514       \n",
      "=================================================================\n",
      "Total params: 2,326,082\n",
      "Trainable params: 2,326,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "Image:  Tensor(\"Reshape:0\", shape=(512, 512, 3), dtype=float32)\n",
      "Image:  Tensor(\"Reshape:0\", shape=(512, 512, 3), dtype=float32)\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[32,64,508,508] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/conv2d_125/Relu (defined at <ipython-input-72-93bf7a920543>:11) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_50706]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-93bf7a920543>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m history = vgg_model.fit(\n\u001b[1;32m---> 11\u001b[1;33m     get_dataset(TRAINING_FILENAMES, True, True), epochs=EPOCHS, callbacks = [sv,get_lr_callback(BATCH_SIZE)], steps_per_epoch=count_data_items(TRAINING_FILENAMES)/BATCH_SIZE, validation_data=get_dataset(VALIDATION_FILENAMES, False, False), verbose=VERBOSE)\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loading best model...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ettore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ettore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ettore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ettore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ettore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ettore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\ettore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[32,64,508,508] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/conv2d_125/Relu (defined at <ipython-input-72-93bf7a920543>:11) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_50706]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "# Save best model\n",
    "sv = tf.keras.callbacks.ModelCheckpoint('vgg_model.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n",
    "with strategy.scope():\n",
    "    # Generate the model:\n",
    "    vgg_model = SequentialNet((512,512,3),2)\n",
    "    vgg_model = vgg_model.build_self(vgg_list)    # Build the model (includes compilation)\n",
    "vgg_model.summary()\n",
    "print('Training...')\n",
    "history = vgg_model.fit(\n",
    "    get_dataset(TRAINING_FILENAMES, True, True), epochs=EPOCHS, callbacks = [sv,get_lr_callback(BATCH_SIZE)], steps_per_epoch=count_data_items(TRAINING_FILENAMES)/BATCH_SIZE, validation_data=get_dataset(VALIDATION_FILENAMES, False, False), verbose=VERBOSE)\n",
    "    \n",
    "print('Loading best model...')\n",
    "vgg_model.load_weights('vgg_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Sequential Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_121 (Conv2D)          (None, 510, 510, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_122 (Conv2D)          (None, 508, 508, 16)      4624      \n",
      "_________________________________________________________________\n",
      "conv2d_123 (Conv2D)          (None, 506, 506, 8)       1160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 506, 506, 32)      288       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 506, 506, 2)       66        \n",
      "=================================================================\n",
      "Total params: 7,034\n",
      "Trainable params: 7,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "Image:  Tensor(\"Reshape:0\", shape=(512, 512, 3), dtype=float32)\n",
      "Image:  Tensor(\"Reshape:0\", shape=(512, 512, 3), dtype=float32)\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[8193152,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/dense_1/Tensordot/MatMul (defined at <ipython-input-73-b83c9c9cfb2f>:11) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_51749]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-b83c9c9cfb2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m history = seq_model.fit(\n\u001b[1;32m---> 11\u001b[1;33m     get_dataset(TRAINING_FILENAMES, True, True), epochs=EPOCHS, callbacks = [sv,get_lr_callback(BATCH_SIZE)], steps_per_epoch=count_data_items(TRAINING_FILENAMES)/BATCH_SIZE, validation_data=get_dataset(VALIDATION_FILENAMES, False, False), verbose=VERBOSE)\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loading best model...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ettore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ettore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ettore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ettore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ettore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ettore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\ettore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[8193152,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/dense_1/Tensordot/MatMul (defined at <ipython-input-73-b83c9c9cfb2f>:11) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_51749]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "# Save the best model\n",
    "sv = tf.keras.callbacks.ModelCheckpoint('seq_model.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n",
    "with strategy.scope():\n",
    "    # Generate the model:\n",
    "    seq_model = SequentialNet((512,512,3),2)\n",
    "    seq_model = seq_model.build_self(seq_list)\n",
    "seq_model.summary()  \n",
    "print('Training...')\n",
    "history = seq_model.fit(\n",
    "    get_dataset(TRAINING_FILENAMES, True, True), epochs=EPOCHS, callbacks = [sv,get_lr_callback(BATCH_SIZE)], steps_per_epoch=count_data_items(TRAINING_FILENAMES)/BATCH_SIZE, validation_data=get_dataset(VALIDATION_FILENAMES, False, False), verbose=VERBOSE)\n",
    "    \n",
    "print('Loading best model...')\n",
    "model.load_weights('fold-%i.h5'%fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Visualization (test tfrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(threshold=15, linewidth=80)\n",
    "CLASSES = [0,1]\n",
    "\n",
    "def batch_to_numpy_images_and_labels(data):\n",
    "    images, labels = data\n",
    "    numpy_images = images.numpy()\n",
    "    numpy_labels = labels.numpy()\n",
    "    #if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n",
    "    #    numpy_labels = [None for _ in enumerate(numpy_images)]\n",
    "    # If no labels, only image IDs, return None for labels (this is the case for test data)\n",
    "    return numpy_images, numpy_labels\n",
    "\n",
    "\n",
    "def display_one_flower(image, title, subplot, red=False, titlesize=16):\n",
    "    plt.subplot(*subplot)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    if len(title) > 0:\n",
    "        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
    "    return (subplot[0], subplot[1], subplot[2]+1)\n",
    "    \n",
    "def display_batch_of_images(databatch, predictions=None):\n",
    "    \"\"\"This will work with:\n",
    "    display_batch_of_images(images)\n",
    "    display_batch_of_images(images, predictions)\n",
    "    display_batch_of_images((images, labels))\n",
    "    display_batch_of_images((images, labels), predictions)\n",
    "    \"\"\"\n",
    "\n",
    "    # data\n",
    "    images, labels = batch_to_numpy_images_and_labels(databatch)\n",
    "    if labels is None:\n",
    "        labels = [None for _ in enumerate(images)]\n",
    "        \n",
    "    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n",
    "    rows = int(math.sqrt(len(images)))\n",
    "    cols = len(images)//rows\n",
    "        \n",
    "    # size and spacing\n",
    "    FIGSIZE = 13.0\n",
    "    SPACING = 0.1\n",
    "    subplot=(rows,cols,1)\n",
    "    if rows < cols:\n",
    "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
    "    else:\n",
    "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
    "    \n",
    "    # display\n",
    "    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n",
    "        title = label\n",
    "        print(\"SHAPE: \", image.shape)\n",
    "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n",
    "        subplot = display_one_flower(image, \"img\", subplot, not label, titlesize=dynamic_titlesize)\n",
    "    \n",
    "    #layout\n",
    "    plt.tight_layout()\n",
    "    if label is None and predictions is None:\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    else:\n",
    "        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
